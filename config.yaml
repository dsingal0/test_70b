base_image:
  image: docker.io/baseten/triton_trt_llm:a6aa8eb6ce9371521df166c480e10262cd9c0cf4
  python_executable_path: /usr/bin/python3
environment_variables: {}
external_package_dirs: []
model_metadata:
  tags:
  - text-generation
  - openai-compatible
  engine_repository: baseten/llama-70b_fp8_tp2_i1024_o100_bs32-tllm_0.12.0.dev2024072301
  tokenizer_repository: neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
  tensor_parallel_count: 2
  pipeline_parallel_count: 1
model_name: Llama 3.1 70B Instruct FP8 KV-Cache Reuse Enabled
python_version: py311
requirements:
- tritonclient[all]
- transformers
- truss
resources:
  accelerator: H100:2
  use_gpu: true
runtime:
  predict_concurrency: 32
system_packages: []